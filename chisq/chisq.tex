% !TeX spellcheck = en_US

\documentclass[11pt]{article} % default template
%% Packages
\usepackage{verbatim}
\usepackage[open,openlevel=1]{bookmark}
\usepackage{dirtytalk}
\usepackage{mathtools} %\DeclarePairedDelimiter
\usepackage{dsfont} %indicator
\usepackage{booktabs} % for fancy tables
\usepackage{makecell} % linebreak inside a cell
\usepackage{algorithm} % for algorithm box
\usepackage{algpseudocode} % for algorithm box
\usepackage{tikz}%for figures
\usepackage{subfigure} % subfigures
\usepackage{graphics}%for figures
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{multirow} %table
\usepackage{adjustbox} % table size
\usepackage{float}
\usepackage{footmisc}
\usepackage{color}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{setspace}
\usepackage{xparse}
\usepackage{tcolorbox}
\usepackage{lipsum}
\usepackage{blindtext}

\usepackage[titletoc,title]{appendix}
\usepackage[T1]{fontenc}

\usepackage[square,numbers]{natbib}
%\setcitestyle{numbers, square}

\usepackage{url}
\usepackage[font=small,labelfont=bf]{caption}
\captionsetup[table]{skip=0.5pt}
\usepackage{authblk} % authors
\usepackage{hyperref}[] % hyperlink
\hypersetup{
colorlinks = true,
linkcolor = blue, %Colour of internal links
filecolor = magenta,
urlcolor = blue, %Colour for external hyperlinks
citecolor = blue, %Colour of citations
pdfpagemode=UseOutlines
}
\usepackage{cleveref} % for hyperinks


\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%align page break
\allowdisplaybreaks[1]
\allowbreak

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


% 1. basic math
% 2. norms
% 3. probability
% 4. minimax rates

% 1. basic math
\newcommand{\vecBold}[1]{\boldsymbol{#1}} %vector notation
\newcommand{\naturalNumber}{\mathbb{N}^{+}}
\newcommand{\real}{\mathbb{R}} %set of real numbers
\newcommand{\floor}[1]{\lfloor #1 \rfloor} %floor function
\newcommand{\bfloor}[1]{\Bigl\lfloor #1 \Bigr\rfloor} %big floor function
\newcommand{\indicator}[1]{\mathds{1} \left( #1 \right) }%indicator funcion
\newcommand{\sampleSize}{n}
%stat
\newcommand{\distparamMultinom}{\boldsymbol{p}}

% 2. norms and distances
\newcommand{\normProbVec}[3]{\|p_{#3}\|_{#1}^{#2}}
\newcommand{\distProbVec}[4]{\|p_{#3} - p_{#4}\|_{#1}^{#2}}
\newcommand{\Lone}{\mathbb{L}_1} %L1 norm
\newcommand{\Ltwo}{\mathbb{L}_2} %L2 norm
\newcommand{\Linfty}{\mathbb{L}_{\infty}} %inf norm
\newcommand{\vertiii}[1]{
	{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}
    }%triple norm notation
    
%lemma and theorem reference
\newcommand{\improvedTwoMomentMethod}{Lemma C.1 of \citet{Kim2022MinimaxTests}}
\newcommand{\normNegBesov}[2]{\|#1 \|^{#2}_{B^{-\gamma}_{2,2}}}
\newcommand{\normNegBesovTrunc}[2]{\widetilde{\|#1 \|^{#2}}_{B^{-\gamma}_{2,2}}}

% 3. probabilities
\newcommand{\mE}{\mathbb{E}} %expectation
\newcommand{\mP}{\mathbb{P}} %probability
\newcommand{\iid}{\stackrel{\text{i.i.d}}{\sim}} %iid


% 4. minimax
\newcommand{\minimaxTestingRateTwosample}{\rho^{\ast}_{n_1, n_2}}

\newcommand{\rvLap}{W}
\newcommand{\realizedLap}{w}

%parameters
\newcommand{\binNum}{\kappa}
\newcommand{\binNumOptimalTs}{\binNum_{(1)}}
\newcommand{\binNumOptimalIndep}{\binNum_{(2)}}
\newcommand{\discLapParam}{\zeta}
\newcommand{\smoothness}{s}
\newcommand{\ballRadius}{R}


%indexing
\newcommand{\sampleIndexY}{i}
\newcommand{\sampleIndexZ}{j}
\newcommand{\alphabetSize}{k}
\newcommand{\alphabetSizeY}{\alphabetSize_Y}
\newcommand{\alphabetSizeZ}{\alphabetSize_Z}
\newcommand{\dimDensity}{d}
\newcommand{\vectorIndex}{m}
\newcommand{\haarScaleSecondIndex}{v}
\newcommand{\adaptiveBinNumIndex}{t}
\newcommand{\mixtureIndex}{\boldsymbol{\eta}}
\newcommand{\adaptiveBinNumIndexSet}{[\gammamax]}

% prob, expectation, variance
\newcommand{\mEQPYPZ}{\mathbb{E}_{(Q_{P_Y}, Q_{P_Z})}}
\newcommand{\mEQPY}{\mathbb{E}_{Q_{P_Y}}}
\newcommand{\mEQPZ}{\mathbb{E}_{Q_{P_Z}}}


\newcommand{\mEQ}{\mathbb{E}_{P, Q}}
\newcommand{\mVQ}{\mathrm{Var}_{P,Q}}
\newcommand{\noiseVarIndep}[1]{\hat{\sigma}^{#1}_\alpha}


% Besov
\newcommand{\besovParamNorm}{p}
\newcommand{\besovParamMicroscope}{q}
\newcommand{\resLev}{j}
\newcommand{\primResLev}{J}
\newcommand{\primResLevOptimalTs}{J_{(1)}}
\newcommand{\wavFatherUnivIndex}{k}
\newcommand{\wavFatherIndex}{\boldsymbol{\wavFatherUnivIndex}}
\newcommand{\wavMotherUnivIndex}{\ell}
\newcommand{\wavMotherIndex}{\boldsymbol{\wavMotherUnivIndex}}
\newcommand{\wavMotherBooleanUnivIndex}{\epsilon}
\newcommand{\wavMotherBooleanIndex}{\boldsymbol{\wavMotherBooleanUnivIndex}}
\newcommand{\wavMotherBooleanIndexSet}{S_{\dimDensity}}
\newcommand{\wavCoef}{\theta}
\newcommand{\wavCoefFather}[1]{\wavCoef_{\primResLev, #1}}
\newcommand{\wavCoefFatherVec}[1]{\wavCoef_{#1}}
\newcommand{\wavCoefMother}[3]{ \wavCoef_{#1, #2}^{#3} }
\newcommand{\wavCoefMotherVec}[1]{\wavCoef_{#1}}
\newcommand{\besovSubscript}{\tilde{\mathcal{B}}_{\besovParamNorm,\besovParamMicroscope}^\smoothness}
%
\newcommand{\wavFatherFunc}{\phi}
\newcommand{\haarScaleIndices}[2]{\wavFatherFunc_{#1, #2}}
\newcommand{\multivInhomoWavFatherBasis}{\overline{\Phi}_{\primResLev}}
\newcommand{\wavGenericFatherCoef}{\wavCoef_{\wavFatherFunc}}
%
\newcommand{\wavMotherFunc}{\psi}
\newcommand{\wavMotherFuncPerturb}{\tilde{\psi}}
\newcommand{\haarMotherIndices}[3]{\wavMotherFunc_{#1, #2}^{#3}}
\newcommand{\multivInhomoWavMotherBasis}{\overline{\uppercase{\Psi}}_{\resLev}} %wavelet function basis subset at resolution level j
\newcommand{\wavGenericMotherCoef}{\wavCoef_{\wavMotherFunc}}

\newcommand{\besovCoeffVec}{{\theta}_{\resLev \cdot }}
%adaptive
\newcommand{\adaptiveSingleTest}{\Delta^{{\adaptiveBinNumIndex}}_ {\gamma/\gammamax}}
\newcommand{\adaptiveBinNumIndexProof}{\tau}


%upper bound
\newcommand{\hyperRec}[1]{\mathrm{H}_{#1}}
\newcommand{\binMNProbVecY}{\hat{p}_{\vecBold{Y}}}
\newcommand{\binMNProbVecZ}{\hat{p}_{\vecBold{Z}}}
\newcommand{\binMNProbVecYZ}{\hat{p}_{\vecBold{YZ}}}
%mathmatics

%dimensions


\newcommand{\genrr}{\mathcal{M}_{\texttt{GenRR}}}
%dimension-related
\newcommand{\vectorElements}[1]{
(#1_{1}, \ldots, #1_{\dimDensity})
}
\newcommand{\dimProd}[1]{\prod_{{#1}=1}^\dimDensity}
\newcommand{\kappaProd}{{\kappa}^\dimDensity}
\newcommand{\kappaProdHalf}{{\kappa}^{\dimDensity/2}}
\newcommand{\kappaProdInv}{{\kappa}^{-\dimDensity}}
\newcommand{\domainTs}{
[0,1]^{{\dimDensity}}
}
\newcommand{\domainIndep}{
[0,1]^{{\dimDensity_1+\dimDensity_2}}
}

%function spaces
\newcommand{\LinftySpace}{\mathbb{L}_{\infty}(\domainTs)}
\newcommand{\LtwoSpace}{\mathbb{L}_{2}(\domainTs)}


\newcommand{\besovBall}[2]{\tilde{\mathcal{B}}_{#1,#2}^\smoothness(\ballRadius)}

\newcommand{\holderTwosample}{\mathcal{C}^{\smoothness}} %new
\newcommand{\holderIndep}{\mathcal{H}^{\dimDensity_1 + \dimDensity_2}_\smoothness(\ballRadius)}
\newcommand{\holderBall}{\mathcal{C}^{\smoothness}(\ballRadius)} %new
\newcommand{\holderTwosampleArias}{\mathcal{H}^\dimDensity_\smoothness(\ballRadius)}

% density spaces
%Holder classes

\newcommand{\pBesovTs}{
	\mathcal{P}_{%subscript
		\text{Besov}
		}^{%superscript
		(\dimDensity, \smoothness, \besovParamMicroscope)
		}
}
\newcommand{\pBesovIndep}{
	\mathcal{P}_{%subscript
		\text{Besov}
		}^{%superscript
		(\dimDensity_1+\dimDensity_2, \smoothness, \besovParamMicroscope)
		}
}
\newcommand{\pHolderTwosample}{
	\mathcal{P}^{(\dimDensity, \smoothness)}_{\text{H\"{o}lder}}
	}
\newcommand{\pHolderIndep}{
	\mathcal{P}^{(\dimDensity_1 + \dimDensity_2, \smoothness)}_{\text{H\"{o}lder}}
	}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Besov



\newcommand{\bumpBasisBesovTwosample}[1]{
\psi^{\boldsymbol{\epsilon}^\ast}_{#1, \boldsymbol{\ell}}
}

\newcommand{\besovBasis}[3]{
\psi_{#1, \boldsymbol{#2}}^{\boldsymbol{#3}}
}

\newcommand{\besovCoeff}[3]{
\theta^{\boldsymbol{#3}}_{#1,\boldsymbol{#2}}
}
%besov indices
\newcommand{\besovEpsilonIndex}{
\{0,1\}^\dimDensity \backslash \{(0, \ldots, 0)\}
}

\newcommand{\besovLambda}[1]{
\{
0, 1, \ldots, (2^{#1} - 1)
\}^{\dimDensity}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Holder



\newcommand{\bumpBasisHolderTwosample}[1]{
\varphi_{(#1, \boldsymbol{\ell})}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%U-statistic
\newcommand{\indepSigmaU}{
	\sum_{ (i_1, i_2, i_3, i_4) \in \mathbf{i}_4^n}
}

\newcommand{\indepSigmaUThree}{
	\sum_{ (i_1, i_2, i_3) \in \mathbf{i}_3^n}
}
%
\newcommand{\indepSigmaUTwo}{
	\sum_{ (i_1, i_2) \in \mathbf{i}_2^n}
}
%common
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}





%%two sample testing

%domain



%kappa
\newcommand{\kappaTsHalf}[1]{
	\kappa^{#1d/2}
	}
\newcommand{\kappaTs}[1]{
	\kappa^{{#1}d}
	}



%privatized vectors

\newcommand{\yTildei}{\widetilde{Y}_{i}}
\newcommand{\zTildei}{\widetilde{Z}_{i}}
\newcommand{\yTildePrimei}{\widetilde{Y}'_{i}}
\newcommand{\zTildePrimei}{\widetilde{Z}'_{i}}


%two sample testing

% distribution space
\newcommand{\pTwosampleDisc}{\mathcal{P}^{(\alphabetSize)}}

% noise variance
\newcommand{\LapUParam}{\sigma_{\alpha}}
%% optimal kappa for two sample testing upper bound
\newcommand{\optimalKappaTwosample}{\kappa^\ast}

%privatized vectors
\newcommand{\elemRandomPrivTwoSampleY}{{\Phi}_{i,\vectorIndex}}
\newcommand{\elemRandomPrivTwoSampleZ}{{\Psi}_{j,\vectorIndex'}}
\newcommand{\elemRandomPrivTwoSampleYNumber}[2]{{\Phi}_{#1, #2}}
\newcommand{\elemRandomPrivTwoSampleZNumber}[2]{{\Psi}_{#1, #2}}
\newcommand{\elemFixedPrivTwoSampleY}{{\phi}_{i, \vectorIndex}}
\newcommand{\elemFixedPrivTwoSampleZ}{{\psi}_{j, \vectorIndex'}}


\newcommand{\vecRandomPrivTwoSampleY}{\boldsymbol{\Phi}_{i}}
\newcommand{\vecRandomPrivTwoSampleZ}{\boldsymbol{\Psi}_{j}}
\newcommand{\vecFixedPrivTwoSampleY}{\boldsymbol{\phi}_{i}}
\newcommand{\vecFixedPrivTwoSampleZ}{\boldsymbol{\psi}_{j}}
\newcommand{\vecFixedPrivTwoSampleYNumber}[1]{\boldsymbol{\phi}_{#1}}
\newcommand{\vecFixedPrivTwoSampleZNumber}[1]{\boldsymbol{\psi}_{#1}}
\newcommand{\vecRandomPrivTwoSampleYNumber}[1]{\boldsymbol{\Phi}_{#1}}
\newcommand{\vecRandomPrivTwoSampleZNumber}[1]{\boldsymbol{\Psi}_{#1}}



\newcommand{\vecRandomPrivAdaptiveY}{\tilde{\boldsymbol{\Phi}}_{i}}
\newcommand{\vecRandomPrivAdaptiveZ}{\tilde{\boldsymbol{\Psi}}_{j}}
\newcommand{\vecFixedPrivAdaptiveY}{\tilde{\boldsymbol{\phi}}_{i}}
\newcommand{\vecFixedPrivAdaptiveZ}{\tilde{\boldsymbol{\psi}}_{j}}

\newcommand{\elemRandomPrivAdaptiveY}{\tilde{\boldsymbol{\Phi}}_{i, \adaptiveBinNumIndex}}
\newcommand{\elemRandomPrivAdaptiveZ}{\tilde{\boldsymbol{\Psi}}_{j, \adaptiveBinNumIndex}}
\newcommand{\elemFixedPrivAdaptiveY}{ \tilde{\boldsymbol{\phi}}_{i, \adaptiveBinNumIndex, \vectorIndex}}
\newcommand{\elemFixedPrivAdaptiveZ}{\tilde{\boldsymbol{\psi}}_{j, \adaptiveBinNumIndex,\vectorIndex}}

\newcommand{\vecRandomPrivIndepY}{\boldsymbol{\Phi}_{i}}
\newcommand{\vecRandomPrivIndepZ}{\boldsymbol{\Psi}_{i}}


%U-statistic
\newcommand{\kernelTwoSample}{h_{ts}}
\newcommand{\kernelTwoSampleSym}{\bar{h}_{ts}}
\newcommand{\kernelIndep}{h_{in}}
\newcommand{\kernelIndepSym}{\bar{h}_{in}}







% two moments method
\newcommand{\momentTwosampleOneY}{M_{Y,1}(P)}
\newcommand{\momentTwosampleOneZ}{M_{Z,1}(P)}
\newcommand{\momentTwosampleYZ}{M_{Y Z,2}(P)}



\newcommand{\LDPviewSingleGeneric}[1]{\tilde{X}_#1}
\newcommand{\LDPviewsGeneric}{(\LDPviewSingleGeneric{1}, \ldots \LDPviewSingleGeneric{n})}
\newcommand{\LDPviewSingleGenericVec}[1]{\tilde{\vecBold{X}}_#1}
\newcommand{\LDPviewGenericVec}{(\LDPviewSingleGenericVec{1}, \ldots \LDPviewSingleGenericVec{n})}

\newcommand{\beforNoiseIndepY}{
	\widetilde{Y}_{i,k}
}

\newcommand{\beforNoiseIndepZ}{
	\widetilde{Z}_{i,j}
}


\newcommand{\priVecIndepRealization}{
\left(
	\phi_{i},
	\psi_{i}
\right)
}

\newcommand{\priVecIndepRV}{
\left(
	\Phi_{i, \kappa},
	\Psi_{i, \kappa}
\right)
}

\newcommand{\priVecIndepDataset}{
\left\{
	\priVecIndep
\right\}_{i=1}^n
}

%kappas
\newcommand{\kappaIndepHalf}[1]{
	\kappa^{\frac{{#1}(d_1+d_2)}{2}}
	}

\newcommand{\kappaIndep}[1]{
	\kappa^{{#1}(d_1+d_2)}
	}


%sigma
\newcommand{\sigmaIndep}[1]{
	\sigma_{\kappa, \alpha}^{#1}
}



%indicator functions
\newcommand{\indicatorIndepY}{
	\mathds{1}\bigl(\widetilde{Y}_i = k\bigr)
}

\newcommand{\indicatorIndepYPrime}{
	\mathds{1}\bigl(\widetilde{Y'}_i = k\bigr)
}

\newcommand{\indicatorIndepZ}{
	\mathds{1}\bigl(\widetilde{Z}_j = k\bigr)
}

\newcommand{\indicatorIndepZPrime}{
	\mathds{1}\bigl(\widetilde{Z'}_j = k\bigr)
}

\newcommand{\dimIndep}{
	\kappa^{d_1+d_2}
}

\newcommand{\scalingIndep}{
	\kappa^{\frac{d_1+d_2}{2}}
}


%lower bound proofs


% Adaptive tests
\newcommand{\gammamax}{
	\mathcal{N}
	}

\newcommand{\adaptiveTest}{
	\Delta_{\mathrm{adapt}}
}


% for lower bound
\newcommand{\mPPrivate}[1]{
	\mP_{{Q, {#1}}}^{(n)}
	}

\newcommand{\mEPrivate}[1]{
	\mE_{{Q, {#1}}}^{(n)}
	}
	
\newcommand{\mPNuRho}{
	\mP_{\nu_{\rho}}
	}
	


%%%%%%%%%%%%%%%%%%% SETS %%%%%%%%%%%%%%%%%%%
\newcommand{\sampleSets}[3]{\{{#1}_{#2}\}_{#2 \in [#3]}}
\newcommand{\laplaceSets}[4]{\{{#1}_{#2, #3}\}_{#3 \in [#4]}}

% set of distributions
\newcommand{\pNull}{\mathcal{P}_0}
\newcommand{\pAlterTwosample}{\mathcal{P}_1(\rhoTwosample)}
\newcommand{\pAlterIndep}{\mathcal{P}_1(\rho_{n})}

\newcommand{\rhoTwosample}{\rho_{n_1, n_2}}




% probability spaces
\newcommand{\sigmaAlgebraYPriv}{\mathcal{F}_{i}^\Phi}

% datasets
\newcommand{\datasetTwosampleY}{\mathcal{Y}_{n_1}}
\newcommand{\datasetTwosampleZ}{\mathcal{Z}_{n_2}}
\newcommand{\datasetIndep}{\mathcal{X}_{n}}
%algorithm names
\newcommand{\permuteTestTS}{\text{\textsc{PermuTestTS}}}
\newcommand{\permuteTestIndep}{\text{\textsc{PermuTestIndep}}}
\newcommand{\privatizer}{\text{\textsc{LapU}}}
\newcommand{\privatizerDisc}{\text{\textsc{DiscLapU}}}
\newcommand{\privatizerAdapt}{\text{\textsc{LapUAdapt}}}
\newcommand{\procedureTSDisc}{\text{\textsc{PrivUTS}}}
\newcommand{\procedureTSConti}{\text{\textsc{PrivUTS}}_\text{\textsc{H\"{o}lder}}}
\newcommand{\procedureTSAdapt}{\text{\textsc{PrivUTS}}_\text{\textsc{Adapt}}}
\newcommand{\procedureIndepMarginal}{ \text{\textsc{PrivUIndep}} }
\newcommand{\procedureIndepSplit}{ \text{\textsc{PrivUSplit}} }


%LapU


%binning function
\newcommand{\binner}[2]{ h^{(#1, #2)}_{\mathrm{bin}} }

% noise variances



\newcommand{\LapUDiscParam}{ p_{\alpha, c, \theta}}
\newcommand{\LapUIndepParam}{\sigma_{\alpha, c, \theta}}
\newcommand{\LapUIndepParamProof}{\sigma_{\alpha, 4, d^\ell}}
\newcommand{\sigmaAlphaKappa}[1]{
	\sigma_{\alpha, \kappa}^{#1}
}




% matrices
\newcommand{\yUmat}{[\gammamax]}
\newcommand{\zUmat}{\mathbf{L}}
\newcommand{\yUmatTilde}{\mathbf{\tilde{K}}}
\newcommand{\zUmatTilde}{\mathbf{\tilde{L}}}




%privatized vectors

\newcommand{\YPrivVec}[1]{\boldsymbol{\Phi}_{#1}}
\newcommand{\ZPrivVec}[1]{\boldsymbol{\Psi}_{#1}}


\newcommand{\YPriv}{\Phi_i}
\newcommand{\YPrivComp}[1]{\Phi_{i, #1}}
\newcommand{\YPrivOne}[1]{\Phi_{1, #1}}

\newcommand{\yPriv}{\phi_i}
\newcommand{\yPrivComp}[1]{\phi_{i, #1}}

\newcommand{\ZPriv}{\Psi_i}
\newcommand{\ZPrivComp}[1]{\Psi_{i, #1}}
\newcommand{\ZPrivOne}[1]{\Psi_{1, #1}}

\newcommand{\zPriv}{\psi_i}
\newcommand{\zPrivComp}[1]{\psi_{i, #1}}

\newcommand{\psionek}{\psi_{1k}}

%probability vector components
\newcommand{\pYk}{p_{Y}(k)}
\newcommand{\pZk}{p_{Z}(\vectorIndex')}
\newcommand{\pYZkk}{p_{YZ}(\vectorIndex, \vectorIndex')}
\newcommand{\pYpZkk}{p_{Y}(k) p_{Z}(\vectorIndex')}
\newcommand{\deltakk}{\Delta_{k,k'}}

%norms
\newcommand{\normpy}{\| p_{Y} \|_2}
\newcommand{\normpz}{\| p_{Z} \|_2}
\newcommand{\normpzsq}{\| p_{Z} \|_2^2}
\newcommand{\normdeltats}{\| p_{Y} - p_Z \|_2}
\newcommand{\normdeltatssq}{\| p_{Y} - p_Z \|_2^2}


%moments
\newcommand{\momentOneIndep}{M'_1(P)}

%independence testing


% definition of noise-added phi

\newcommand{\ik}{h_{in}}
\newcommand{\PsiIndepOne}{\Psi'_1}

%norms
\newcommand{\normpyz}{\|P_{\vecBold{YZ}}\|_2}
\newcommand{\normpyzsq}{\|P_{\vecBold{YZ}}\|_2^2}
\newcommand{\normpypz}{\|p_{Y}p_{Z}\|_2}
\newcommand{\normpypzsq}{\|p_{Y}p_{Z}\|_2^2}
\newcommand{\normdelta}{\|P_{\vecBold{YZ}} - p_Y p_Z\|_2}



% lower bound proofs
\newcommand{\intdone}{
\int_{[0,1]^{\dimDensity_1}}
	}
	
\newcommand{\intdtwo}{
\int_{[0,1]^{\dimDensity_2}}
	}

\newcommand{\intindepwhole}{\int_{[0,1]^{\dimDensity_1 + \dimDensity_2}}}	
\newcommand{\qDensityY}{
q^Y_i(\phi_i|y_i)
	}

\newcommand{\qDensityZ}{
		q^Z_i(\psi_i|z_i)
	}	
	
\newcommand{\qDensityYPrime}{
q^Y_i(\phi_i|y_i')
	}

\newcommand{\qDensityZPrime}{
		q^Z_i(\psi_i|z_i')
	}	

\newcommand{\qDensityYCdot}{
q^Y_i(\cdot|y_i)
	}

\newcommand{\qDensityZCdot}{
		q^Z_i(\cdot|z_i)
	}

\newcommand{\privateUnif}{
\tilde{f}_{0, i}
	}

\newcommand{\dMuY}{
d\mu^Y_i(\phi_i)
	}

\newcommand{\dMuZ}{
d\mu^Z_i(\psi_i)
	}






\newcommand{\rvY}{Y}
\newcommand{\rVecY}{\vecBold{\rvY}}
\newcommand{\rvZ}{Z}
\newcommand{\rVecZ}{\vecBold{\rvZ}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mV}{\mathcal{V}}
\newcommand{\tV}{\text{Var}}

\newcommand{\bX}{\textbf{X}}
\newcommand{\bY}{\textbf{Y}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\bZ}{\textbf{Z}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bcdot}{\boldsymbol{\cdot}}
\newcommand{\bell}{\boldsymbol{\ell}}



\newcommand{\convAS}{\overset{a.s}{\longrightarrow}}
\newcommand{\convP}{\overset{p}{\longrightarrow}}
\newcommand{\convD}{\overset{d}{\longrightarrow}}
\newcommand\fnurl[2]{%
\href{#2}{#1}\footnote{\url{#2}}%
}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}} % independece
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
% \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\setlength{\oddsidemargin}{0in} \setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in} \setlength{\topmargin}{-.2in}
\setlength{\textheight}{8.25in}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{property}{Property}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

%comments
\newcommand{\ilmun}[1]{
	{ \color{blue} Ilmun: #1}
	}

\newcommand{\jongmin}[1]{
	{ \color{red} #1}
	}
	
\newcommand{\sw}[1]{
    { \color{green} S: #1}
    }
    
\newcommand{\sobolevTwo}[1]{\mathcal{W}_1^{#1}(\Omega)}
\newcommand{\advLossSobol}[1]{d_{#1}^\mathcal{W}}

\begin{document}

\begin{center}
\LARGE \bf
LDP two-sample  chi-squared test
\end{center}

\begin{comment}
\begin{keyword}
\kwd{local differential privacy}
\kwd{two-sample testing}
\kwd{independence testing}
\kwd{minimax separation rates}
\end{keyword}

%\end{frontmatter} #uncomment this to use the Bernoulli template
\end{comment}
%%%%%%%%% uncomment the codes above to use Bernoulli template (Part II)%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{2}
\section{Setting}
\begin{itemize}
	\item $
\rVecY_i \stackrel{iid}{\sim}multi(\sampleSize_1, \distparamMultinom_{\rVecY}),
\rVecZ_i \stackrel{iid}{\sim}multi(\sampleSize_2, \distparamMultinom_{\rVecZ})$ with $\alphabetSize$ categories
	\item One-hot vector form i.e. random vectors with dependent Bernoulli random variable entries
	\item Allow for $n_1 \neq n_2$
	
\end{itemize}
\section{Generalized Randomized Response and two sample Pearson chi-square statistic}
\subsection{Privacy mechanism: Generalized Randomized Response}
\begin{definition}[Generalized Randomized Response~(Theorem 5.4. of \citet{Gaboardi2018LDPChisq})]
For a multinomial random vector 
$
\rVecY_i \stackrel{iid}{\sim}multi(\sampleSize_1, \distparamMultinom_{\rVecY})$, 
we define
\begin{align*}
\mP
\bigl(
\genrr(\rVecY_i) = \vecBold{y}'
|
\rVecY_i = \vecBold{y}
\bigr)
:=
\begin{cases}
\dfrac
	{\exp(\alpha)}
	{\exp(\alpha) + \alphabetSize - 1 }
	\text{ if }
	\vecBold{y}' = \vecBold{y}
	\\
\dfrac
	{1}
	{\exp(\alpha) + \alphabetSize - 1 }	
	\text{ if }
	\vecBold{y}' \neq \vecBold{y}.
\end{cases}
\end{align*}
Then  $\tilde{\rVecY}_i:=\genrr(\rVecY_i)$ is a multinomial random vector with probability vector
\begin{align*}
\tilde{\distparamMultinom}_{\rVecY}
:=
\distparamMultinom_{\rVecY}
	\frac
	{\exp(\alpha)}
	{\exp(\alpha) + \alphabetSize - 1 }
	+
(1-\distparamMultinom_{\rVecY})	
	\frac
	{1}
	{\exp(\alpha) + \alphabetSize - 1 }.
\end{align*}
\end{definition}
\noindent
Since $e^\alpha>1$ for $\alpha>0$, the probability of sending the original category is a little bit higher than sending the other category.
\citet{Gaboardi2018LDPChisq} constructs a private goodness-of-fit test based on a chi-square statistic evaluated on $\tilde{\rVecY}_i$'s.
They demonstrate that the limiting distribution is chi-square distribution both under the null and alternative.



\subsection{Two sample chi-square statistic}
We extend the goodness-of-fit test by \citet{Gaboardi2018LDPChisq} into two-sample testing by privatizing the raw samples
$\rVecZ_i \stackrel{iid}{\sim}multi(\sampleSize_2, \distparamMultinom_{\rVecZ})$
into $\tilde{\rVecZ}_j := \genrr(\rVecZ_j)$. 
Under the null, $\genrr(\rVecY_i)$ and $\genrr(\rVecZ_j)$ follow multinomial distributions with the same probability vector.
 Therefore, the usual two-sample chi-square test statistic
 	\begin{align*}
		T_{\chi} :=
		\sum_{\ell=1}^k
		\frac{
		\bigl(
		n_2
		\sum_{i=1}^{n_1}\tilde{\rVecY}_i(\ell)
		-
		n_1
		\sum_{j=1}^{n_1}\tilde{\rVecZ}_j(\ell)
		\bigr)^2
		}{
		n_1 n_2 (n_1 + n_2) 
		\sum_{j=1}^{n_1}
		\bigl(
		\tilde{\rVecY}_j(\ell)+\tilde{\rVecZ}_j(\ell)
		\bigr)
		}
	\end{align*}
  converges to a chi-square distribution with degree of freedom $\alphabetSize-1$ and yields a valid  test with size $\gamma$.
This test statistic is from Van der Vaart's book Asymptotic Statistics, pp. 253.

\section{Bit flip privatization and related test statisitc}
\subsection{Bit flip privatization}
\subsection{test statistic}
\begin{lemma}[Lemma 5.7 of \cite{Gaboardi2018LDPChisq}]\label{lemma_57}
When $X_i \stackrel{iid}{\sim} multinomial(\boldsymbol{p}, 1)$, denote the histogram of flipped observations as $\tilde{\boldsymbol{H}}:= \sum_{i=1}^n \mathcal{M}_{bit}(X_i)$. The mean vector and covariance matrices are computed as follows: 
\begin{equation}
	\tilde{\boldsymbol{p}}:=	
	\mathbb{E}(\mathcal{M}_{bit}(X_1))
	=
	\frac{(\exp
		\bigl(
		\alpha/2)-1
		\bigr) \boldsymbol{p}+1}{\exp(\alpha/2)+1}, \text{ and}
\end{equation}

\begin{equation}
	\Sigma_{\boldsymbol{p}}
	:=
	Var(\mathcal{M}_{bit}(X_1))
	=
	\left(
	\frac{\exp(\alpha/2)-1}{\exp(\alpha/2)+1}
	\right)^2
	\bigl(
	diag(\boldsymbol{p}) - \boldsymbol{p}\boldsymbol{p}^\top
	\bigr)
	+
	\frac{\exp(\alpha/2)}{(\exp(\alpha/2)+1)^2}
	I_d,
\end{equation}
For any $\alpha >0$ and $\boldsymbol{p} >0$, $\Sigma_{\boldsymbol{p}}$ is full-rank  and one of its eigenvector is one-vector. 
By the CLT for i.i.d random vectors, we get the following asymptotic distribution:
\begin{equation}
	\sqrt{n}(\tilde{\boldsymbol{H}}/n - \tilde{\boldsymbol{p}})
	\stackrel{d}{\to}
	N
	\bigl(
	0, \Sigma_{\boldsymbol{p}}
	\bigr)
\end{equation}
\end{lemma}
We can extend this lemma to two-sample setting.
Suppose
 $Y_i \stackrel{iid}{\sim} multinomial(\boldsymbol{p}_1, 1)$
 and
  $Z_j \stackrel{iid}{\sim} multinomial(\boldsymbol{p}_2, 1)$.
We follow Lemma~\ref{lemma_57} to denote
$\tilde{\boldsymbol{p}}_Y = \mathbb{E}(\mathcal{M}_{bit}(Y_1)), 
\Sigma_{\boldsymbol{p}_Y} := Var(\mathcal{M}_{bit}(Y_1))$
and
$\tilde{\boldsymbol{p}}_Z = \mathbb{E}(\mathcal{M}_{bit}(Z_1)),
\Sigma_{\boldsymbol{p}_Z} := Var(\mathcal{M}_{bit}(Z_1))$.
Denote 
$\tilde{Y_i} := \mathcal{M}_{bit}(Y_i) - \tilde{\boldsymbol{p}}_Y$
and
$\tilde{Z_j} := \mathcal{M}_{bit}(Z_j) - \tilde{\boldsymbol{p}}_Z$.
Then denote
$T_n := \sum_{i=1}^n \tilde{Y_i} - \sum_{j=1}^n \tilde{Z_j}$
and
$\Sigma_n := Var(T_n) =  n \bigl( \Sigma_{\boldsymbol{p}_Y} + \Sigma_{\boldsymbol{p}_Z} \bigr)$.

Under the null hypothesis of $\boldsymbol{p}_Y = \boldsymbol{p}_Z = \boldsymbol{p}$, 
we have
$T_n = \sum_{i=1}^n\mathcal{M}_{bit}(Y_i) - \sum_{j=1}^n \mathcal{M}_{bit}(Z_j)= \tilde{\boldsymbol{H}_Y} - \tilde{\boldsymbol{H}_Z} $ and 
$\Sigma_n = 2n \Sigma_{\boldsymbol{p}}$.
So we have
\begin{equation}
	\sqrt{n/2}(\tilde{\boldsymbol{H}_Y}/n -\tilde{\boldsymbol{H}_Z}/n)
	\stackrel{d}{\to}
	N
	\bigl(
	0, \Sigma_{\boldsymbol{p}}
	\bigr)
\end{equation}
Since $\Sigma_{\boldsymbol{p}}$ is symmetric and one of its eigenvector is one-vector, we can diagonalize it as  $\Sigma_{\boldsymbol{p}} = BDB^\top$, where $D$ is a diagonal matrix and $B$ has orthogonal columns with one of them being $k^{-1} \boldsymbol{1}$.

We introduce $\Pi := I_d - \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T$.
First, this is an orthogornal projection matrix, since it is symmetric and idempotent:
\begin{align*}
\Pi^2 =
\left(
I_d - \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
\right)
\left(
I_d - \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
\right)
&=
I_d -  \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
 -  \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
 +
 \frac{1}{k^2}\boldsymbol{1} \boldsymbol{1}^T\boldsymbol{1} \boldsymbol{1}^T
\\&=
I_d -  2\frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
+
\frac{1}{k^2}\boldsymbol{1} (\boldsymbol{1}^T\boldsymbol{1}) \boldsymbol{1}^T
\\&=
I_d -  2\frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
+
\frac{1}{k^2}\boldsymbol{1} (k \boldsymbol{1}^T)
\\&=
I_d -  \frac{1}{k}\boldsymbol{1} \boldsymbol{1}^T
\\&= \Pi.
\end{align*}
Second, its null space is $span\{\boldsymbol{1}\}$:
\begin{align*}
	\Pi x  = 0
	&\iff
	x = (1/k) \boldsymbol{1} \boldsymbol{1}^\top x
	\\&
	\iff
x = (1/k) \boldsymbol{1} (\boldsymbol{1}^\top x) = ((\boldsymbol{1}^\top x)/k) \boldsymbol{1} = c\boldsymbol{1}
\end{align*}
$\boldsymbol{1} \boldsymbol{1}^T$
Since $\Pi$ is symmetric, its column space is the orthogonal complement of $span\{\boldsymbol{1}\}$.
So multiplying by $\Pi$ means 
under the null, it suffices to use the CLT for i.i.d. random vectors, but under the alternative, we would need to use Lindeburg or Lyapunov.










\begin{align*}
	\frac{\tilde{\boldsymbol{H}}_1}{n_1} - \frac{\tilde{\boldsymbol{H}}_2}{n_2}
\end{align*}

\bibliographystyle{apalike}
\bibliography{reference}


\end{document}
